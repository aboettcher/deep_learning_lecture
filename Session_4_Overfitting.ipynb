{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST('~/Downloads/mnist', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('~/Downloads/mnis', download=True, train=False, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aboettcher/.miniconda3/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(0, len(trainset.train_labels), 1) < 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset.targets = trainset.targets[idx]\n",
    "trainset.data = trainset.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 30\n",
       "    Root location: /Users/aboettcher/Downloads/mnist\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=2000, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=2000, out_features=2000, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=2000, out_features=10, bias=True)\n",
      "  (9): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 784\n",
    "hidden_sizes = [2000, 2000]\n",
    "output_size = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_number = 0\n",
    "trainset[image_number][0].numpy().shape  # C x W x H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[image_number][0].numpy().flatten().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot a sample from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a75171eca9542cf832a0d16e6c1ec6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='image_number', max=20), Output()), _dom_classes=('widgetâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_sample(image_number=0)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_sample(image_number=0):\n",
    "    f, axs = plt.subplots(1, 2 ,figsize=(10, 5))\n",
    "    axs[0].imshow(trainset[image_number][0][0].numpy())\n",
    "    axs[0].set_title(f\"label: {trainset[image_number][1]}\")\n",
    "    axs[0].set_axis_off()\n",
    "    axs[1].bar(range(10), (model(trainset[image_number][0].reshape((1, 784)))).detach().numpy()[0])\n",
    "    axs[1].set_ylabel(\"prediction probablity\")\n",
    "    axs[1].set_xlabel(\"class\")\n",
    "    plt.show()\n",
    "interact(plot_sample, image_number=(0, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see the prediction of the number with random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(10), (model(trainset[image_number][0].reshape((1, 784)))).detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 2.265017032623291, Val loss: 2.2962800757900164\n",
      "Epoch 10 - Training loss: 2.1033496856689453, Val loss: 2.231686857855244\n",
      "Epoch 20 - Training loss: 1.964534878730774, Val loss: 2.1858101771895293\n",
      "Epoch 30 - Training loss: 1.8455528020858765, Val loss: 2.118870968271972\n",
      "Epoch 40 - Training loss: 1.7673945426940918, Val loss: 2.0994830898418546\n",
      "Epoch 50 - Training loss: 1.7113745212554932, Val loss: 2.088088748561349\n",
      "Epoch 60 - Training loss: 1.6673202514648438, Val loss: 2.0367823322867133\n",
      "Epoch 70 - Training loss: 1.6204400062561035, Val loss: 2.0094991428836897\n",
      "Epoch 80 - Training loss: 2.327808380126953, Val loss: 2.3628228561134095\n",
      "Epoch 90 - Training loss: 2.327817440032959, Val loss: 2.362822857631999\n",
      "Epoch 100 - Training loss: 2.327817440032959, Val loss: 2.362822857631999\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-689a36ab4e86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.5, momentum=0.5)\n",
    "\n",
    "n_epochs = 200\n",
    "for epoch_number in range(n_epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0], -1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = loss_function(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        if epoch_number % 10 == 0:\n",
    "            val_running_loss = 0.\n",
    "            for images, labels in valloader:\n",
    "                images = images.view(images.shape[0], -1)\n",
    "                output = model(images)\n",
    "                loss = loss_function(output, labels)\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "            print(\"Epoch {} - Training loss: {}, Val loss: {}\".format(epoch_number, running_loss/len(trainloader), val_running_loss/len(valloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check again the prediction with the trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(image_number=0):\n",
    "    f, axs = plt.subplots(1, 2 ,figsize=(10, 5))\n",
    "    axs[0].imshow(trainset[image_number][0][0].numpy())\n",
    "    axs[0].set_title(f\"label: {trainset[image_number][1]}\")\n",
    "    axs[0].set_axis_off()\n",
    "    axs[1].bar(range(10), (model(trainset[image_number][0].reshape((1, 784)))).detach().numpy()[0])\n",
    "    axs[1].set_ylabel(\"prediction probablity\")\n",
    "    axs[1].set_xlabel(\"class\")\n",
    "interact(plot_sample, image_number=(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(image_number=0):\n",
    "    f, axs = plt.subplots(1, 2 ,figsize=(10, 5))\n",
    "    axs[0].imshow(valset[image_number][0][0].numpy())\n",
    "    axs[0].set_title(f\"label: {valset[image_number][1]}\")\n",
    "    axs[0].set_axis_off()\n",
    "    axs[1].bar(range(10), (model(valset[image_number][0].reshape((1, 784)))).detach().numpy()[0])\n",
    "    axs[1].set_ylabel(\"prediction probablity\")\n",
    "    axs[1].set_xlabel(\"class\")\n",
    "interact(plot_sample, image_number=(0, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit (conda)",
   "language": "python",
   "name": "python37464bitconda1afd2bebd57c4058801075be7a010ae1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
